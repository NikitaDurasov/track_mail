{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Reinforcement Learning (DQN)\n",
    "=====================================\n",
    "\n",
    "\n",
    "Cart-Pole\n",
    "----------\n",
    "** Цель ** - удержать шест в вертикальном положении. Агент воздействует на среду двумя действиями:\n",
    "* движение влево\n",
    "* движение вправо\n",
    "\n",
    "Агент наблюдает текущее стсояние среды и выбирает действие, после чего среда переходит в новое состяние, и возвращает награду. В этом примере игра заканчивается, если шест упал слишком сильно\n",
    "\n",
    "В задаче среда поставляет агенту четыре числа:\n",
    "* позиция\n",
    "* скорость\n",
    "* угол ....\n",
    "\n",
    "С точки зрения же DQN мы решаем эту задачу через взгляд на скрин, который и является состоянием.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Replay Memory\n",
    "\n",
    "\n",
    "Мы используем \"experience replay memory\" для тренировки нашей сети. \n",
    "В нее мы сохраняем все переходы, которые делает агент для последующей тренировки сети. Основная цель - это декорреляция примеров в батчах.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Положить переход в память.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Получить сэмпл из памяти \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "DQN алгоритм\n",
    "-------------\n",
    "\n",
    "Наша цель найти такую политику, которая пытается максимизировать дисконтированную кумулятивную награду\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, \n",
    "$R_{t_0}$ - ответ среды. The discount,\n",
    "$\\gamma$, постоянное числов диапазоне от $0$ до $1$, что бы сумма сходилась. \n",
    "Такая награда говорит, что чем дальше в будущее мы заглядываем тем менее оно для нас ценно.\n",
    "\n",
    "Идея Q-learning - мы имеем функцию оценки ценности действия \n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, которая говорит нам какая награда будет если мы выберем действие в текущем состоянии. Таким образом, мы можем сконструйировать политику, которая максимизирует нашу награду:\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "Мы не имеем прямого доступа к  $Q^*$, но мы можем сделать апроксимацию нашей функции при помощи нейросети и получить нашу \n",
    "$Q^*$.\n",
    "\n",
    "Во время тренировки мы используем тот факт, что функция $Q$\n",
    "для политики соответсвует уравнению Беллмана:\n",
    "\n",
    "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
    "\n",
    "Разница между правой и левой частью уравнения - это ошибка временной разницы $\\delta$:\n",
    "\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
    "\n",
    "Для минимизации используем [Функция потерь Хьюбера](https://en.wikipedia.org/wiki/Huber_loss). \n",
    "Функция действует как MSE если ошибка небольшая и L1 если ошибка большая. Это позволяет сделать функцию потерь усточивой к выбросам, так как оцениваемая $Q$ слишком зашумленная. Мы считаем ошибку по батчам $B$, которые сэмплируются из replay memory:\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{для } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{иначе.}\n",
    "   \\end{cases}\\end{align}\n",
    "\n",
    "### Q-network\n",
    "\n",
    "Модель - это конволюционная сеть, которая получает на вход разницу между текущим и предыдущим патчем скрина. Сеть имеет два выхода, которые представляют  $Q(s, \\mathrm{лево})$ и $Q(s, \\mathrm{право})$ (где $s$ вход сети). Сеть пытается запредиктить ценность действия, которое опреднеляется текущим входом.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        #self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        #self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        #self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Вход\n",
    "\n",
    "\n",
    "Вытаскиваем картинку из среды. Используем бонусы из ``torchvision``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADWCAYAAADBwHkCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE7qZWUH8Pww0PsPyM4RZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bf157f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Scale(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # транспонирование в порядок торча (СHW)\n",
    "    # Убираем верх и низ экрана\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Убираем края экрана, чтобы получить картинку с центрированной тележкой\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Конвертируем в торч тензор\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Ресайзим и добавляем батч размерность\n",
    "    return resize(screen).unsqueeze(0).type(Tensor)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "          interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "#### Гиперпараметры и утилиты\n",
    "\n",
    "\n",
    "-  ``select_action`` - выбор действия согласно $\\epsilon$ - жадной политике. Сэмплируем равномерно. Начальное значение вероятности задается ``EPS_START`` и уменьшается экспоненциально до ``EPS_END``. ``EPS_DECAY`` - задает клэффициент убывания\n",
    "-  ``plot_durations`` - строит график продолжительности эпизодов, и среднее по 100 последним эпизодам.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "\n",
    "model = DQN()\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return model(\n",
    "            Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Тренировка модели\n",
    "\n",
    "\n",
    "Выбирает батчи, склеиваем тензора в один пакет, считаем $Q(s_t, a_t)$ и\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, комбинируем их с лосом.\n",
    "Устанавливаем $V(s) = 0$ if $s$ если узел терминальный.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "last_sync = 0\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    global last_sync\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    # выбираем новый батч\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Для всех состояний считаем маску не финальнсти и конкантенируем их\n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)))\n",
    "\n",
    "    # Блокируем прохождение градиента для вычисления функции ценности действия\n",
    "    # volatile=True\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]),\n",
    "                                     volatile=True)\n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "\n",
    "    # Считаем Q(s_t, a) - модель дает Q(s_t), затем мы выбираем\n",
    "    # колоки, которые соответствуют нашим действиям на щаге\n",
    "    state_action_values = model(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Подсчитываем ценность состяония V(s_{t+1}) для всех последующмх состояний.\n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor))\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0] # берем значение максимума\n",
    "    \n",
    "    # Для подсчета лоса нам нужно будет разрешить прохождение градиента по переменной\n",
    "    # блокировку, которого мы унаследовали\n",
    "    # requires_grad=False\n",
    "    next_state_values.volatile = False\n",
    "    # Считаем ожидаемое значение функции оценки ценности действия  Q-values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Считаем ошибку Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Оптимизация модели\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Тренируем модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115607cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "the given numpy array has zero-sized dimensions. Zero-sized dimensions are not supported in PyTorch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c43ed8589d91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Получаем новое состояние\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlast_screen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcurrent_screen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_screen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6a7485dc2804>\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Конвертируем в торч тензор\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Ресайзим и добавляем батч размерность\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the given numpy array has zero-sized dimensions. Zero-sized dimensions are not supported in PyTorch"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115607cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "for i_episode in range(num_episodes):\n",
    "    # Инициализация среды\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        \n",
    "        # Выбрать и выполнить нове действие\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action[0, 0])\n",
    "        reward = Tensor([reward])\n",
    "\n",
    "        # Получаем новое состояние\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Сохраняем состояние, следующее состояние, награду и действие в память\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Переходим в новое состояние\n",
    "        state = next_state\n",
    "\n",
    "        # Шаг оптимизации \n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "kernel35",
   "language": "python",
   "name": "kernel35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
